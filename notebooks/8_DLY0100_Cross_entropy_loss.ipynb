{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Explicaci√≥n cross entropy loss\n",
        "\n",
        "**Autor:** Jazna Meza Hidalgo\n",
        "\n",
        "**Correo Electr√≥nico:** ja.meza@profesor.duoc.cl\n",
        "\n",
        "**Fecha de Creaci√≥n:** Febrero de 2025    \n",
        "**Versi√≥n:** 1.0  \n",
        "\n",
        "---\n",
        "\n",
        "## Descripci√≥n\n",
        "\n",
        "Este notebook explica el concepto de la entropia cruzada (cross entropy).\n",
        "\n",
        "*Cross entropy* es una funci√≥n de p√©rdida utilziada en problemas de clasificaci√≥n, especialmente en modelos que producen probabilidades, como las redes neuronales con **Softmax** en la √∫ltima capa.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Requisitos de Software\n",
        "\n",
        "Este notebook fue desarrollado con Python 3.9. A continuaci√≥n se listan las bibliotecas necesarias:\n",
        "\n",
        "- numpy (1.26.4)\n",
        "\n",
        "Para verificar la versi√≥n instalada ejecutar usando el nombre del paquete del cual quieres saber la versi√≥n; por ejemplo, si quieres saber la versi√≥n de sklearn usas:\n",
        "\n",
        "```bash\n",
        "import numpy\n",
        "print(numpy.__version__)\n",
        "````"
      ],
      "metadata": {
        "id": "5jkMuYXQ2Pbo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intuici√≥n detr√°s de cross entropy\n",
        "\n",
        "Cuando se entrena un modelo de clasificaci√≥n, se quiere que la probabilidad predicha para la clase correcta sea la m√°s **alta posible**.\n",
        "\n",
        "Por ejemplo, si se est√°n clasificando im√†genes de gatos y perror, un buen modelo debe asignar una **probabilidad alta** a la clase en cada caso.\n",
        "\n",
        "Si un modelo predice que una imagen de un gato tiene una probabilidad de 0.9 de ser gato y 0.1 de ser perro, es una mejor predicci√≥n que decir que ambas clases tienne 0,5 de probabilidad.\n",
        "\n",
        "*Cross entropy* mide **qu√© tan bien est√°n alineadas las probabilidades predichas con la clase real**"
      ],
      "metadata": {
        "id": "F6ZB1SmP3IRz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Matem√°tica asociada con cross entropy\n",
        "\n",
        "Para una √∫nica muestra con una distribuci√≥n real y una distrobuci√≥n predicha $\\hat{y}$ la **cross entropy** se define como:\n",
        "\n",
        "$H(y,\\hat{y})=-\n",
        "\\sum_{i=1}^{C}y_{i}log(\\hat{y})$\n",
        "\n",
        "Donde:\n",
        "\n",
        "+ C es el n√∫mero de clases.\n",
        "+ $y_i$ es el **valor real** de la clase $i$ (1 es la clase correcta, 0 en caso contrario).\n",
        "+ $\\hat{y}$ es la **probabilidad predicha** por el modelo para la clase $i$.\n",
        "\n",
        "Si s√≥lo se tienen **una clase correcta**, esta ecuaci√≥n se reduce a:\n",
        "\n",
        "$L=-log(\\hat{y}_{correcta})$\n"
      ],
      "metadata": {
        "id": "2r4HjKZK3-4o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejemplo con 3 clases (gatos, perro, panda)\n",
        "\n",
        "| Clase         | Probabilidad predicha $\\hat{y}_i$|\n",
        "|---------------|----------------------------------|\n",
        "| Gato (0)      |                         0.1      |\n",
        "| Perro (1)     |                         0.7      |\n",
        "| Panda (2)     |                         0.2      |\n",
        "\n",
        "Como la imagen es un perro, la entrop√≠a cruzada ser√°:\n",
        "\n",
        "$L=-log(0.7)$\n",
        "\n",
        "Si el modelo fuera perro y predijera 0.3 para perro:\n",
        "\n",
        "$L=-log(0.3)$\n",
        "\n",
        "üîπ Cuanto mayor sea la probabilidad predicha de la clase correcta, menor ser√° la p√©rdida.\n",
        "\n",
        "üîπ Si la probabilidad de la clase correcta es cercana a 0, la p√©rdida ser√° muy alta."
      ],
      "metadata": {
        "id": "UQAmGF1e4CxS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejemplo pr√°ctico"
      ],
      "metadata": {
        "id": "lg76qf8m3EZc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtD5zged2CPH",
        "outputId": "a7d81589-d613-4a08-cbd5-63e20b0c90e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P√©rdida (modelo bueno): 0.35667494393873245\n",
            "P√©rdida (modelo malo) : 1.2039728043259361\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def cross_entropy(y_true: np.array, y_pred: np.array):\n",
        "    \"\"\"\n",
        "    Calcula la p√©rdida de entrop√≠a cruzada entre etiquetas reales y predichas.\n",
        "\n",
        "    Par√°metros:\n",
        "    - y_true: np.array de etiquetas reales (one-hot encoded)\n",
        "    - y_pred: np.array de probabilidades predichas (softmax output)\n",
        "\n",
        "    Retorna:\n",
        "    - P√©rdida de entrop√≠a cruzada promedio.\n",
        "    \"\"\"\n",
        "    epsilon = 1e-12  # Para evitar log(0)\n",
        "    y_pred = np.clip(y_pred, epsilon, 1. - epsilon)\n",
        "    return -np.sum(y_true * np.log(y_pred)) / y_true.shape[0]\n",
        "\n",
        "# Ejemplo: Clasificaci√≥n de 3 clases\n",
        "y_real = np.array([[0, 1, 0]])  # La clase real es la segunda (Perro)\n",
        "y_pred_bien = np.array([[0.1, 0.7, 0.2]])  # Buen modelo\n",
        "y_pred_mal = np.array([[0.3, 0.3, 0.4]])  # Mal modelo\n",
        "\n",
        "print(f\"P√©rdida (modelo bueno): {cross_entropy(y_real, y_pred_bien)}\")\n",
        "print(f\"P√©rdida (modelo malo) : {cross_entropy(y_real, y_pred_mal)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "**Comentarios**\n",
        "\n",
        "---\n",
        "\n",
        "üîπ El modelo bueno tendr√° menor p√©rdida que el malo.\n",
        "\n",
        "üîπ Si la clase correcta tiene una probabilidad alta, la p√©rdida ser√° baja.\n",
        "\n"
      ],
      "metadata": {
        "id": "6GTY5B6B75bZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ¬øPor qu√© se usa en clasificaci√≥n?\n",
        "\n",
        "+ Es **convexa**, lo que facilita la optimizaci√≥n con m√©todos como **descenso de gradiente**.\n",
        "+ **Maximiza la probabilidad de la clase correcta**, lo que hace que el modelo aprenda a generar predicciones m√°s confiables.\n",
        "+ Funciona bien con **softmax**, ya que este convierte los valores en una distriuci√≥n de probabilidad."
      ],
      "metadata": {
        "id": "IFW4o4Yc8Ef4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resumen\n",
        "\n",
        "‚úÖ Mide qu√© tan bien las probabilidades predichas se alinean con las etiquetas reales.\n",
        "\n",
        "‚úÖ Si la clase correcta tiene probabilidad baja, la p√©rdida ser√° grande.\n",
        "\n",
        "‚úÖ Es una funci√≥n clave en clasificaci√≥n y se usa junto con Softmax.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "üîπ üí° Entre m√°s confianza tenga el modelo en la clase correcta, menor ser√° la p√©rdida.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "mt5P95nb8sQg"
      }
    }
  ]
}